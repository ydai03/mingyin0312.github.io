<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Ming  Yin | Variance Reduction Technique for Optimal Offline RL</title>
<meta name="description" content="Personal page for Ming Yin
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/blog/2021/doubleVR/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    
  </head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "Variance Reduction Technique for Optimal Offline RL",
      "description": "A algorithm that achieves Minimax rate for tabular RL",
      "published": "March 4, 2021",
      "authors": [
        
        {
          "author": "Ming Yin",
          "authorURL": "",
          "affiliations": [
            {
              "name": "CS, UCSB",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Ming</span>   Yin
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              Blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Publications
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                Teaching/Talks
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                Zproject
                
              </a>
          </li>
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="post distill">

      <d-title>
        <h1>Variance Reduction Technique for Optimal Offline RL</h1>
        <p>A algorithm that achieves Minimax rate for tabular RL</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <p>The discussion is based on <d-cite key="yin2021nearoptimal"></d-cite>.</p>

<h2 id="brief-background-of-offline-learning">Brief background of Offline Learning</h2>

<p>Historical data $\mathcal{D}=\left\lbrace (s_t^{(i)},a_t^{(i)},r_t^{(i)})\right\rbrace_{i\in[n]}^{t\in[H]} $ was obtained by logging policy $\mu$ and we can only use $\mathcal{D}$ to estimate the value of target policy $\pi$, <em>i.e.</em> $v^\pi$. Suppose we only assume knowledge about $\pi$ and $r_t^{(i)} = r_t(s_t^{(i)},a_t^{(i)})$. The goal of offline learning task is to find an <em>$\epsilon$-optimal policy</em> $\pi_\text{out}$, such that</p>

\[\left\lVert V_1^{\pi^\star}-V_1^{\pi_\text{out}}\right\rVert_\infty&lt;\epsilon.\]

<p>In particular, <d-cite key="yin2021near"></d-cite> obtains the $\tilde{O}(H^3/d_m\epsilon^2)$ complexity and <d-cite key="yin2021nearoptimal"></d-cite> further tightens the result to $\tilde{O}(H^2/d_m\epsilon^2)$ via a <em>Variance Reducetion</em> based algorithm.</p>

<h2 id="a-brief-review-of-variance-reduction-for-rl">A brief review of Variance Reduction for RL</h2>

<p>In the case of policy optimization, VR is an algorithm that approximately iterating the Bellman optimality equation, using an inner loop that performs an approximate value (or Q-value) iteration using fresh interactive data to estimate $V^\star$, and an outer loop that performs multiple steps of such iterations to refine the estimates. Concretely, to obtain an reliable $Q_t(s,a)$ for some step $t\in[H]$, by the Bellman equation $Q_t(s,a)=r(s,a)+P_t^\top(\cdot \mid s,a)V_{t+1}$, we need to estimate $P_t^\top(\cdot\mid s,a)$ with sufficient accuracy. VR handles this by decomposing:</p>

<p>\begin{equation}\label{eq:VR_decomposition}
    \quad P_t^\top(\cdot|s,a)V_{t+1} 
     =P_t^\top(\cdot|s,a)(V_{t+1}-V_{t+1}^{\text{in}})+P_t^\top(\cdot|s,a)V_{t+1}^{\text{in}},
\end{equation}</p>

<p>where $V_{t+1}^{\text{in}}$ is a <em>reference</em> value function obtained from previous calculation and $P_t^\top(\cdot\mid s,a)(V_{t+1}-V_{t+1}^{\text{in}})$, $P_t^\top(\cdot\mid s,a)V_{t+1}^{\text{in}}$ are estimated separately at different stages. This technique can help in reducing the <em>effective variance</em> along the learning process.</p>

<hr />

<h2 id="hightlights-of-our-results">Hightlights of our results</h2>

<p>In particular, we design the <em>Off-Policy Double Variance Reduction</em> (<strong>OPDVR</strong>) algorithm to achieve the following:</p>

<ul>
    <li>For finite horizon non-stationary transition (time-variant) setting, OPDVR outputs a $\epsilon$-optimal policy with complexity $\tilde{O}(H^3/d_m\epsilon^2)$;
   </li>
    <li>
    For finite horizon stationary transition (time-invariant) setting, OPDVR outputs a $\epsilon$-optimal policy with complexity $\tilde{O}(H^2/d_m\epsilon^2)$;
    </li>
    <li>
    	For infinite horizon discounted setting, OPDVR outputs a $\epsilon$-optimal policy with complexity $\tilde{O}((1-\gamma)^{-3}/d_m\epsilon^2)$;
	</li>
</ul>
<p>All of above have minimax rate in their respective settings! If you are interested, please check <d-cite key="yin2021nearoptimal"></d-cite> for a reference.</p>

<hr />

<h2 id="miscellaneous">Miscellaneous</h2>

<p>My nice collaborator also shared this on twitter:</p>
<div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet" data-width="500"><p lang="en" dir="ltr">New preprint on offline RL:<a href="https://t.co/2vv2KLA1TF">https://t.co/2vv2KLA1TF</a><br /><br />* A variance reduction algorithm for offline RL<br />* Optimal horizon dependence: O(H^2/d_m) sample complexity on time-homogeneous MDPs<br /><br />Joint w/ Ming Yin (<a href="https://twitter.com/MingYin_0312?ref_src=twsrc%5Etfw">@MingYin_0312</a>) and Yu-Xiang Wang</p>&mdash; Yu Bai (@yubai01) <a href="https://twitter.com/yubai01/status/1358887058274570241?ref_src=twsrc%5Etfw">February 8, 2021</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2022 Ming  Yin.
    
    
    
  </div>
</footer>



  </body>

  <d-bibliography src="/assets/bibliography/doubleVR-distill.bib">
  </d-bibliography>

</html>
